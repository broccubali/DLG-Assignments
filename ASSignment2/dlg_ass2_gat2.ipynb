{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch_geometric\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Open</th>\n",
       "      <th>Open.1</th>\n",
       "      <th>Open.2</th>\n",
       "      <th>Open.3</th>\n",
       "      <th>Open.4</th>\n",
       "      <th>Open.5</th>\n",
       "      <th>Open.6</th>\n",
       "      <th>Open.7</th>\n",
       "      <th>...</th>\n",
       "      <th>Volume.10</th>\n",
       "      <th>Volume.11</th>\n",
       "      <th>Volume.12</th>\n",
       "      <th>Volume.13</th>\n",
       "      <th>Volume.14</th>\n",
       "      <th>Volume.15</th>\n",
       "      <th>Volume.16</th>\n",
       "      <th>Volume.17</th>\n",
       "      <th>Volume.18</th>\n",
       "      <th>Volume.19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>36.944462</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>34.342203</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-04</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>34.473390</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-07</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>35.468021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-08</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>35.673153</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11083</th>\n",
       "      <td>2022-12-23</td>\n",
       "      <td>XOM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>11539400.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11084</th>\n",
       "      <td>2022-12-27</td>\n",
       "      <td>XOM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>11962100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11085</th>\n",
       "      <td>2022-12-28</td>\n",
       "      <td>XOM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>10702100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11086</th>\n",
       "      <td>2022-12-29</td>\n",
       "      <td>XOM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>10534000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11087</th>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>XOM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>11799600.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20160 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date Ticker       Open  Open.1  Open.2  Open.3  Open.4  Open.5  \\\n",
       "0     2019-01-02   AAPL  36.944462     NaN     NaN     NaN     NaN     NaN   \n",
       "1     2019-01-03   AAPL  34.342203     NaN     NaN     NaN     NaN     NaN   \n",
       "2     2019-01-04   AAPL  34.473390     NaN     NaN     NaN     NaN     NaN   \n",
       "3     2019-01-07   AAPL  35.468021     NaN     NaN     NaN     NaN     NaN   \n",
       "4     2019-01-08   AAPL  35.673153     NaN     NaN     NaN     NaN     NaN   \n",
       "...          ...    ...        ...     ...     ...     ...     ...     ...   \n",
       "11083 2022-12-23    XOM        NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "11084 2022-12-27    XOM        NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "11085 2022-12-28    XOM        NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "11086 2022-12-29    XOM        NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "11087 2022-12-30    XOM        NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "\n",
       "       Open.6  Open.7  ...   Volume.10  Volume.11  Volume.12  Volume.13  \\\n",
       "0         NaN     NaN  ...         NaN        NaN        NaN        NaN   \n",
       "1         NaN     NaN  ...         NaN        NaN        NaN        NaN   \n",
       "2         NaN     NaN  ...         NaN        NaN        NaN        NaN   \n",
       "3         NaN     NaN  ...         NaN        NaN        NaN        NaN   \n",
       "4         NaN     NaN  ...         NaN        NaN        NaN        NaN   \n",
       "...       ...     ...  ...         ...        ...        ...        ...   \n",
       "11083     NaN     NaN  ...  11539400.0        NaN        NaN        NaN   \n",
       "11084     NaN     NaN  ...  11962100.0        NaN        NaN        NaN   \n",
       "11085     NaN     NaN  ...  10702100.0        NaN        NaN        NaN   \n",
       "11086     NaN     NaN  ...  10534000.0        NaN        NaN        NaN   \n",
       "11087     NaN     NaN  ...  11799600.0        NaN        NaN        NaN   \n",
       "\n",
       "       Volume.14  Volume.15  Volume.16  Volume.17  Volume.18  Volume.19  \n",
       "0            NaN        NaN        NaN        NaN        NaN        NaN  \n",
       "1            NaN        NaN        NaN        NaN        NaN        NaN  \n",
       "2            NaN        NaN        NaN        NaN        NaN        NaN  \n",
       "3            NaN        NaN        NaN        NaN        NaN        NaN  \n",
       "4            NaN        NaN        NaN        NaN        NaN        NaN  \n",
       "...          ...        ...        ...        ...        ...        ...  \n",
       "11083        NaN        NaN        NaN        NaN        NaN        NaN  \n",
       "11084        NaN        NaN        NaN        NaN        NaN        NaN  \n",
       "11085        NaN        NaN        NaN        NaN        NaN        NaN  \n",
       "11086        NaN        NaN        NaN        NaN        NaN        NaN  \n",
       "11087        NaN        NaN        NaN        NaN        NaN        NaN  \n",
       "\n",
       "[20160 rows x 102 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('train_stock_data.csv')\n",
    "train_data['Date'] = pd.to_datetime(train_data['Date'], format='%Y-%m-%d')\n",
    "train_data.sort_values(['Ticker', 'Date'], inplace=True)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = pd.read_csv('validation_stock_data.csv')\n",
    "with open('hyperedges.json', 'r') as f:\n",
    "    hyperedges = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>36.944462</td>\n",
       "      <td>37.889005</td>\n",
       "      <td>36.787037</td>\n",
       "      <td>37.667179</td>\n",
       "      <td>148158800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>34.342203</td>\n",
       "      <td>34.757230</td>\n",
       "      <td>33.869933</td>\n",
       "      <td>33.915253</td>\n",
       "      <td>365248800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-04</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>34.473390</td>\n",
       "      <td>35.432244</td>\n",
       "      <td>34.299271</td>\n",
       "      <td>35.363071</td>\n",
       "      <td>234428400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-07</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>35.468021</td>\n",
       "      <td>35.499030</td>\n",
       "      <td>34.800162</td>\n",
       "      <td>35.284359</td>\n",
       "      <td>219111200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-08</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>35.673153</td>\n",
       "      <td>36.212212</td>\n",
       "      <td>35.425093</td>\n",
       "      <td>35.956993</td>\n",
       "      <td>164101200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20155</th>\n",
       "      <td>2022-12-23</td>\n",
       "      <td>XOM</td>\n",
       "      <td>99.121032</td>\n",
       "      <td>100.780001</td>\n",
       "      <td>99.074697</td>\n",
       "      <td>100.724396</td>\n",
       "      <td>11539400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20156</th>\n",
       "      <td>2022-12-27</td>\n",
       "      <td>XOM</td>\n",
       "      <td>101.271219</td>\n",
       "      <td>102.383381</td>\n",
       "      <td>100.863433</td>\n",
       "      <td>102.123878</td>\n",
       "      <td>11962100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20157</th>\n",
       "      <td>2022-12-28</td>\n",
       "      <td>XOM</td>\n",
       "      <td>101.864365</td>\n",
       "      <td>101.947773</td>\n",
       "      <td>100.001499</td>\n",
       "      <td>100.446358</td>\n",
       "      <td>10702100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20158</th>\n",
       "      <td>2022-12-29</td>\n",
       "      <td>XOM</td>\n",
       "      <td>100.084912</td>\n",
       "      <td>101.688276</td>\n",
       "      <td>100.084912</td>\n",
       "      <td>101.206337</td>\n",
       "      <td>10534000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20159</th>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>XOM</td>\n",
       "      <td>100.659532</td>\n",
       "      <td>102.411179</td>\n",
       "      <td>100.659532</td>\n",
       "      <td>102.225822</td>\n",
       "      <td>11799600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20160 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date Ticker        Open        High         Low       Close  \\\n",
       "0     2019-01-02   AAPL   36.944462   37.889005   36.787037   37.667179   \n",
       "1     2019-01-03   AAPL   34.342203   34.757230   33.869933   33.915253   \n",
       "2     2019-01-04   AAPL   34.473390   35.432244   34.299271   35.363071   \n",
       "3     2019-01-07   AAPL   35.468021   35.499030   34.800162   35.284359   \n",
       "4     2019-01-08   AAPL   35.673153   36.212212   35.425093   35.956993   \n",
       "...          ...    ...         ...         ...         ...         ...   \n",
       "20155 2022-12-23    XOM   99.121032  100.780001   99.074697  100.724396   \n",
       "20156 2022-12-27    XOM  101.271219  102.383381  100.863433  102.123878   \n",
       "20157 2022-12-28    XOM  101.864365  101.947773  100.001499  100.446358   \n",
       "20158 2022-12-29    XOM  100.084912  101.688276  100.084912  101.206337   \n",
       "20159 2022-12-30    XOM  100.659532  102.411179  100.659532  102.225822   \n",
       "\n",
       "            Volume  \n",
       "0      148158800.0  \n",
       "1      365248800.0  \n",
       "2      234428400.0  \n",
       "3      219111200.0  \n",
       "4      164101200.0  \n",
       "...            ...  \n",
       "20155   11539400.0  \n",
       "20156   11962100.0  \n",
       "20157   10702100.0  \n",
       "20158   10534000.0  \n",
       "20159   11799600.0  \n",
       "\n",
       "[20160 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = []\n",
    "tickers = train_data['Ticker'].unique()\n",
    "for ticker in tickers:\n",
    "    data = train_data[train_data[\"Ticker\"] == ticker]\n",
    "    valid_cols = []\n",
    "    for i in data.columns:\n",
    "        if data[i].isna().sum() == 0:\n",
    "            valid_cols.append(i)\n",
    "    a = data[valid_cols].values\n",
    "    for i in a:\n",
    "        l.append(i)\n",
    "df = pd.DataFrame(l, columns=[\"Date\", \"Ticker\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.462705</td>\n",
       "      <td>-0.458112</td>\n",
       "      <td>-0.460747</td>\n",
       "      <td>-0.456032</td>\n",
       "      <td>5.101136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.493476</td>\n",
       "      <td>-0.494767</td>\n",
       "      <td>-0.495698</td>\n",
       "      <td>-0.500455</td>\n",
       "      <td>13.554442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-04</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.491925</td>\n",
       "      <td>-0.486867</td>\n",
       "      <td>-0.490554</td>\n",
       "      <td>-0.483313</td>\n",
       "      <td>8.460403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-07</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.480164</td>\n",
       "      <td>-0.486085</td>\n",
       "      <td>-0.484553</td>\n",
       "      <td>-0.484245</td>\n",
       "      <td>7.863964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-08</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-0.477738</td>\n",
       "      <td>-0.477738</td>\n",
       "      <td>-0.477065</td>\n",
       "      <td>-0.476281</td>\n",
       "      <td>5.721920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20155</th>\n",
       "      <td>2022-12-23</td>\n",
       "      <td>XOM</td>\n",
       "      <td>0.272531</td>\n",
       "      <td>0.277970</td>\n",
       "      <td>0.285540</td>\n",
       "      <td>0.290567</td>\n",
       "      <td>-0.218712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20156</th>\n",
       "      <td>2022-12-27</td>\n",
       "      <td>XOM</td>\n",
       "      <td>0.297957</td>\n",
       "      <td>0.296736</td>\n",
       "      <td>0.306972</td>\n",
       "      <td>0.307137</td>\n",
       "      <td>-0.202252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20157</th>\n",
       "      <td>2022-12-28</td>\n",
       "      <td>XOM</td>\n",
       "      <td>0.304971</td>\n",
       "      <td>0.291638</td>\n",
       "      <td>0.296645</td>\n",
       "      <td>0.287275</td>\n",
       "      <td>-0.251315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20158</th>\n",
       "      <td>2022-12-29</td>\n",
       "      <td>XOM</td>\n",
       "      <td>0.283929</td>\n",
       "      <td>0.288601</td>\n",
       "      <td>0.297644</td>\n",
       "      <td>0.296273</td>\n",
       "      <td>-0.257861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20159</th>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>XOM</td>\n",
       "      <td>0.290723</td>\n",
       "      <td>0.297061</td>\n",
       "      <td>0.304529</td>\n",
       "      <td>0.308344</td>\n",
       "      <td>-0.208580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20160 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date Ticker      Open      High       Low     Close     Volume\n",
       "0     2019-01-02   AAPL -0.462705 -0.458112 -0.460747 -0.456032   5.101136\n",
       "1     2019-01-03   AAPL -0.493476 -0.494767 -0.495698 -0.500455  13.554442\n",
       "2     2019-01-04   AAPL -0.491925 -0.486867 -0.490554 -0.483313   8.460403\n",
       "3     2019-01-07   AAPL -0.480164 -0.486085 -0.484553 -0.484245   7.863964\n",
       "4     2019-01-08   AAPL -0.477738 -0.477738 -0.477065 -0.476281   5.721920\n",
       "...          ...    ...       ...       ...       ...       ...        ...\n",
       "20155 2022-12-23    XOM  0.272531  0.277970  0.285540  0.290567  -0.218712\n",
       "20156 2022-12-27    XOM  0.297957  0.296736  0.306972  0.307137  -0.202252\n",
       "20157 2022-12-28    XOM  0.304971  0.291638  0.296645  0.287275  -0.251315\n",
       "20158 2022-12-29    XOM  0.283929  0.288601  0.297644  0.296273  -0.257861\n",
       "20159 2022-12-30    XOM  0.290723  0.297061  0.304529  0.308344  -0.208580\n",
       "\n",
       "[20160 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler \n",
    "scaler = RobustScaler()\n",
    "scaler1 = RobustScaler()\n",
    "feature_cols = ['Open', 'High', 'Low', 'Close']\n",
    "df[feature_cols] = scaler.fit_transform(df[feature_cols])\n",
    "df[\"Volume\"] = scaler1.fit_transform(df[[\"Volume\"]])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.623278</td>\n",
       "      <td>0.612889</td>\n",
       "      <td>0.569116</td>\n",
       "      <td>0.561800</td>\n",
       "      <td>3.697717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.583652</td>\n",
       "      <td>0.586973</td>\n",
       "      <td>0.579894</td>\n",
       "      <td>0.576898</td>\n",
       "      <td>2.801964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-05</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.586458</td>\n",
       "      <td>0.576676</td>\n",
       "      <td>0.576104</td>\n",
       "      <td>0.561215</td>\n",
       "      <td>2.484575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-06</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.573366</td>\n",
       "      <td>0.605831</td>\n",
       "      <td>0.577643</td>\n",
       "      <td>0.615052</td>\n",
       "      <td>2.749050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-09</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.625499</td>\n",
       "      <td>0.641928</td>\n",
       "      <td>0.636862</td>\n",
       "      <td>0.621256</td>\n",
       "      <td>2.088489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>2023-12-22</td>\n",
       "      <td>XOM</td>\n",
       "      <td>0.259961</td>\n",
       "      <td>0.253179</td>\n",
       "      <td>0.267733</td>\n",
       "      <td>0.254458</td>\n",
       "      <td>-0.164882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>2023-12-26</td>\n",
       "      <td>XOM</td>\n",
       "      <td>0.264835</td>\n",
       "      <td>0.254189</td>\n",
       "      <td>0.271178</td>\n",
       "      <td>0.257068</td>\n",
       "      <td>-0.012501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>2023-12-27</td>\n",
       "      <td>XOM</td>\n",
       "      <td>0.256901</td>\n",
       "      <td>0.248805</td>\n",
       "      <td>0.262221</td>\n",
       "      <td>0.251621</td>\n",
       "      <td>-0.101139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>2023-12-28</td>\n",
       "      <td>XOM</td>\n",
       "      <td>0.249535</td>\n",
       "      <td>0.238260</td>\n",
       "      <td>0.248326</td>\n",
       "      <td>0.234940</td>\n",
       "      <td>-0.032197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>2023-12-29</td>\n",
       "      <td>XOM</td>\n",
       "      <td>0.237974</td>\n",
       "      <td>0.226818</td>\n",
       "      <td>0.242470</td>\n",
       "      <td>0.232557</td>\n",
       "      <td>0.022789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date Ticker      Open      High       Low     Close    Volume\n",
       "0    2023-01-03   AAPL  0.623278  0.612889  0.569116  0.561800  3.697717\n",
       "1    2023-01-04   AAPL  0.583652  0.586973  0.579894  0.576898  2.801964\n",
       "2    2023-01-05   AAPL  0.586458  0.576676  0.576104  0.561215  2.484575\n",
       "3    2023-01-06   AAPL  0.573366  0.605831  0.577643  0.615052  2.749050\n",
       "4    2023-01-09   AAPL  0.625499  0.641928  0.636862  0.621256  2.088489\n",
       "...         ...    ...       ...       ...       ...       ...       ...\n",
       "1995 2023-12-22    XOM  0.259961  0.253179  0.267733  0.254458 -0.164882\n",
       "1996 2023-12-26    XOM  0.264835  0.254189  0.271178  0.257068 -0.012501\n",
       "1997 2023-12-27    XOM  0.256901  0.248805  0.262221  0.251621 -0.101139\n",
       "1998 2023-12-28    XOM  0.249535  0.238260  0.248326  0.234940 -0.032197\n",
       "1999 2023-12-29    XOM  0.237974  0.226818  0.242470  0.232557  0.022789\n",
       "\n",
       "[2000 rows x 7 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data = pd.read_csv('validation_stock_data.csv')\n",
    "val_data['Date'] = pd.to_datetime(val_data['Date'], format='%Y-%m-%d')\n",
    "val_data.sort_values(['Ticker', 'Date'], inplace=True)\n",
    "l = []\n",
    "val_tickers = sorted(val_data['Ticker'].unique())\n",
    "for ticker in val_tickers:\n",
    "    data = val_data[val_data[\"Ticker\"] == ticker]\n",
    "    valid_cols = []\n",
    "    for i in data.columns:\n",
    "        if data[i].isna().sum() == 0:\n",
    "            valid_cols.append(i)\n",
    "    a = data[valid_cols].values\n",
    "    for i in a:\n",
    "        l.append(i)\n",
    "df1 = pd.DataFrame(l, columns=[\"Date\", \"Ticker\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"])\n",
    "df1[feature_cols] = scaler.transform(df1[feature_cols])\n",
    "df1[\"Volume\"] = scaler1.transform(df1[[\"Volume\"]])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ticker to index mapping: {'AAPL': 0, 'AMZN': 1, 'BA': 2, 'BAC': 3, 'C': 4, 'CAT': 5, 'CVX': 6, 'DUK': 7, 'GOOGL': 8, 'JNJ': 9, 'JPM': 10, 'KO': 11, 'MRK': 12, 'MSFT': 13, 'PFE': 14, 'PG': 15, 'T': 16, 'VZ': 17, 'WMT': 18, 'XOM': 19}\n"
     ]
    }
   ],
   "source": [
    "stocks = ['AAPL', 'AMZN', 'BA', 'BAC', 'C', 'CAT', 'CVX', 'DUK', 'GOOGL', 'JNJ', 'JPM' ,'KO', 'MRK', 'MSFT', 'PFE', 'PG', 'T', 'VZ', 'WMT', 'XOM']\n",
    "\n",
    "ticker_to_idx = {ticker: idx for idx, ticker in enumerate(sorted(tickers))}\n",
    "print(\"Ticker to index mapping:\", ticker_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperedge 'Tech': ['AAPL', 'GOOGL', 'MSFT', 'AMZN'] -> [0, 8, 13, 1]\n",
      "Hyperedge 'Finance': ['JPM', 'BAC', 'C'] -> [10, 3, 4]\n",
      "Hyperedge 'Healthcare': ['JNJ', 'PFE', 'MRK'] -> [9, 14, 12]\n",
      "Hyperedge 'Energy': ['XOM', 'CVX'] -> [19, 6]\n",
      "Hyperedge 'Consumer': ['WMT', 'PG', 'KO'] -> [18, 15, 11]\n",
      "Hyperedge 'Industrials': ['BA', 'CAT'] -> [2, 5]\n",
      "Hyperedge 'Communications': ['VZ', 'T'] -> [17, 16]\n",
      "Hyperedge 'Utilities': ['DUK'] -> [7]\n"
     ]
    }
   ],
   "source": [
    "hyperedge_indices = []\n",
    "for he_name, ticker in hyperedges.items():\n",
    "    indices = [ticker_to_idx[t] for t in ticker]\n",
    "    print(f\"Hyperedge '{he_name}': {ticker} -> {indices}\")\n",
    "    hyperedge_indices.append(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_matrix = [[0 for i in range(20)] for j in range(20)]\n",
    "for hyperedge in hyperedge_indices:\n",
    "    for i in range(len(hyperedge)):\n",
    "        for j in range(i+1, len(hyperedge)):\n",
    "            adj_matrix[hyperedge[i]][hyperedge[j]] = 1\n",
    "            adj_matrix[hyperedge[j]][hyperedge[i]] = 1\n",
    "adj_matrix = np.array(adj_matrix)\n",
    "adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "edge_index = torch.tensor(np.array(np.nonzero(adj_matrix)), dtype=torch.long)\n",
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_date(date_str):\n",
    "    date = pd.to_datetime(date_str)\n",
    "    return [\n",
    "        date.month / 12,\n",
    "        date.day / 31,\n",
    "        date.weekday() / 6,\n",
    "        np.sin(2 * np.pi * date.dayofyear / 365.25),\n",
    "        np.cos(2 * np.pi * date.dayofyear / 365.25),  \n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_features(stock, date):\n",
    "    stock_idx = ticker_to_idx[stock]\n",
    "    date_features = encode_date(date) \n",
    "    return stock_idx, date_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_for_date(date, df, tickers_today):\n",
    "    stock_feats = []\n",
    "    targets = []\n",
    "    for stock in tickers_today:\n",
    "        row = df[(df['Date'] == date) & (df['Ticker'] == stock)]\n",
    "        if row.empty:\n",
    "            continue\n",
    "        stock_idx, date_feat = get_stock_features(stock, date)\n",
    "        x = date_feat\n",
    "        y = row[['Open', 'High', 'Low', 'Close', 'Volume']].values[0]\n",
    "        stock_feats.append(x)\n",
    "        targets.append(y)\n",
    "    return torch.tensor(stock_feats, dtype=torch.float), torch.tensor(targets, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_edge_index(edge_index, current_tickers, all_tickers):\n",
    "    idx_map = {ticker: i for i, ticker in enumerate(all_tickers)}\n",
    "    local_map = {ticker: i for i, ticker in enumerate(current_tickers)}\n",
    "    \n",
    "    keep_edges = []\n",
    "    for i in range(edge_index.shape[1]):\n",
    "        src_global = edge_index[0, i].item()\n",
    "        dst_global = edge_index[1, i].item()\n",
    "        src_ticker = all_tickers[src_global]\n",
    "        dst_ticker = all_tickers[dst_global]\n",
    "        if src_ticker in current_tickers and dst_ticker in current_tickers:\n",
    "            keep_edges.append((local_map[src_ticker], local_map[dst_ticker]))\n",
    "\n",
    "    if not keep_edges:\n",
    "        return torch.empty((2, 0), dtype=torch.long)\n",
    "    \n",
    "    edge_index_masked = torch.tensor(keep_edges, dtype=torch.long).t().contiguous()\n",
    "    return edge_index_masked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GATConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class GATStockPredictor(nn.Module):\n",
    "    def __init__(self, input_dim=5, hidden_dim=64, out_dim=5, heads=16):\n",
    "        super().__init__()\n",
    "        self.gat1 = GATConv(input_dim, hidden_dim, heads=heads)\n",
    "        self.gat2 = GATConv(hidden_dim * heads, hidden_dim, heads=1)\n",
    "        self.regressor = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.elu(self.gat1(x, edge_index))\n",
    "        x = F.elu(self.gat2(x, edge_index))\n",
    "        return self.regressor(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, optimizer, loss_fn, train_dates, val_dates, edge_index_full, df, df1, all_tickers, device, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        with tqdm(train_dates, desc=f\"Epoch {epoch+1} [Train]\") as t:\n",
    "            for date in t:\n",
    "                tickers_today = sorted(df[df['Date'] == date]['Ticker'].unique())\n",
    "                if not tickers_today:\n",
    "                    continue\n",
    "                x, y = get_features_for_date(date, df, tickers_today)\n",
    "                edge_index_masked = mask_edge_index(edge_index_full, tickers_today, all_tickers)\n",
    "                if edge_index_masked.numel() == 0:\n",
    "                    continue\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                x, edge_index_masked, y = x.to(device), edge_index_masked.to(device), y.to(device)\n",
    "                preds = model(x, edge_index_masked)\n",
    "                loss = loss_fn(preds, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "                t.set_postfix(loss=loss.item())\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            with tqdm(val_dates, desc=f\"Epoch {epoch+1} [Val]\") as t:\n",
    "                for date in t:\n",
    "                    tickers_today = sorted(df1[df1['Date'] == date]['Ticker'].unique())\n",
    "                    if not tickers_today:\n",
    "                        continue\n",
    "                    x, y = get_features_for_date(date, df1, tickers_today)\n",
    "                    edge_index_masked = mask_edge_index(edge_index_full, tickers_today, all_tickers)\n",
    "                    if edge_index_masked.numel() == 0:\n",
    "                        continue\n",
    "                    x, edge_index_masked, y = x.to(device), edge_index_masked.to(device), y.to(device)\n",
    "                    preds = model(x, edge_index_masked)\n",
    "                    loss = loss_fn(preds, y)\n",
    "                    val_loss += loss.item()\n",
    "                    t.set_postfix(val_loss=loss.item())\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1} Summary: Train Loss = {train_loss / len(train_dates):.4f}, Val Loss = {val_loss / len(val_dates):.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_edge_index(tickers):\n",
    "    n = len(tickers)\n",
    "    src = []\n",
    "    dst = []\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                src.append(i)\n",
    "                dst.append(j)\n",
    "    edge_index = torch.tensor([src, dst], dtype=torch.long)\n",
    "    return edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dates = df['Date'].unique()\n",
    "train_dates = sorted(train_dates)\n",
    "val_dates = df1['Date'].unique()\n",
    "train_dates = sorted(train_dates)\n",
    "model = GATStockPredictor(input_dim=5, hidden_dim=128, out_dim=5).to(\"cuda\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]: 100%|██████████| 1008/1008 [00:40<00:00, 25.00it/s, loss=0.588]\n",
      "Epoch 1 [Val]: 100%|██████████| 250/250 [00:02<00:00, 94.28it/s, val_loss=0.723]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary: Train Loss = 0.7798, Val Loss = 0.5594\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Train]: 100%|██████████| 1008/1008 [00:39<00:00, 25.36it/s, loss=0.59] \n",
      "Epoch 2 [Val]: 100%|██████████| 250/250 [00:03<00:00, 63.00it/s, val_loss=0.716]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Summary: Train Loss = 0.7776, Val Loss = 0.5640\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Train]: 100%|██████████| 1008/1008 [00:41<00:00, 24.18it/s, loss=0.591]\n",
      "Epoch 3 [Val]: 100%|██████████| 250/250 [00:03<00:00, 73.60it/s, val_loss=0.708]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Summary: Train Loss = 0.7780, Val Loss = 0.5475\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Train]: 100%|██████████| 1008/1008 [00:41<00:00, 24.20it/s, loss=0.593]\n",
      "Epoch 4 [Val]: 100%|██████████| 250/250 [00:03<00:00, 69.10it/s, val_loss=0.71] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Summary: Train Loss = 0.7775, Val Loss = 0.5358\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Train]: 100%|██████████| 1008/1008 [00:41<00:00, 24.17it/s, loss=0.59] \n",
      "Epoch 5 [Val]: 100%|██████████| 250/250 [00:03<00:00, 77.11it/s, val_loss=0.706]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 Summary: Train Loss = 0.7780, Val Loss = 0.5340\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 [Train]: 100%|██████████| 1008/1008 [00:42<00:00, 23.74it/s, loss=0.593]\n",
      "Epoch 6 [Val]: 100%|██████████| 250/250 [00:03<00:00, 72.19it/s, val_loss=0.707]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 Summary: Train Loss = 0.7777, Val Loss = 0.5404\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 [Train]: 100%|██████████| 1008/1008 [00:42<00:00, 23.56it/s, loss=0.594]\n",
      "Epoch 7 [Val]: 100%|██████████| 250/250 [00:03<00:00, 63.90it/s, val_loss=0.718]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 Summary: Train Loss = 0.7776, Val Loss = 0.5537\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 [Train]: 100%|██████████| 1008/1008 [00:42<00:00, 23.74it/s, loss=0.593]\n",
      "Epoch 8 [Val]: 100%|██████████| 250/250 [00:03<00:00, 81.19it/s, val_loss=0.702]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 Summary: Train Loss = 0.7778, Val Loss = 0.5303\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 [Train]: 100%|██████████| 1008/1008 [00:41<00:00, 24.07it/s, loss=0.591]\n",
      "Epoch 9 [Val]: 100%|██████████| 250/250 [00:02<00:00, 87.52it/s, val_loss=0.706]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 Summary: Train Loss = 0.7783, Val Loss = 0.5344\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 [Train]: 100%|██████████| 1008/1008 [00:40<00:00, 24.89it/s, loss=0.59] \n",
      "Epoch 10 [Val]: 100%|██████████| 250/250 [00:03<00:00, 66.78it/s, val_loss=0.709]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 Summary: Train Loss = 0.7786, Val Loss = 0.5345\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_tickers = sorted((df['Ticker'].unique()))\n",
    "edge_index_full = build_edge_index(all_tickers)\n",
    "\n",
    "train(model, optimizer, loss_fn, train_dates, val_dates, edge_index_full, df, df1, all_tickers, device, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mean_absolute_error, mean_squared_error\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mevaluate\u001b[39m(model, val_dates, df, val_edge_index, tickers, device):\n\u001b[1;32m      4\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/projects/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1343\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1340\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1341\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:930\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 930\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    931\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    933\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1329\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1323\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1324\u001b[0m             device,\n\u001b[1;32m   1325\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1326\u001b[0m             non_blocking,\n\u001b[1;32m   1327\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1328\u001b[0m         )\n\u001b[0;32m-> 1329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "model = model.to(\"cpu\")\n",
    "def evaluate(model, val_dates, df, val_edge_index, tickers, device):\n",
    "    model.eval()\n",
    "    preds_list, actuals_list = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for date in tqdm(val_dates, desc=\"Evaluating\"):\n",
    "            tickers_today = sorted(df[df['Date'] == date]['Ticker'].unique())\n",
    "            if len(tickers_today) == 0:\n",
    "                continue\n",
    "\n",
    "            x, y = get_features_for_date(date, df, tickers_today)\n",
    "            edge_index = mask_edge_index(val_edge_index, tickers_today, tickers)\n",
    "            if edge_index.numel() == 0:\n",
    "                continue\n",
    "            x = torch.tensor(x, dtype=torch.float).to(device)\n",
    "            y = torch.tensor(y, dtype=torch.float).to(device)\n",
    "            edge_index = edge_index.to(device)\n",
    "\n",
    "            x = x.to(device)\n",
    "            edge_index = val_edge_index.to(device)\n",
    "\n",
    "            preds = model(x, edge_index)\n",
    "\n",
    "            preds_list.append(preds.cpu())\n",
    "            actuals_list.append(y)\n",
    "\n",
    "    preds = torch.cat(preds_list, dim=0)\n",
    "    actuals = torch.cat(actuals_list, dim=0)\n",
    "\n",
    "    mask = actuals.abs().sum(dim=1) > 0\n",
    "    preds = preds[mask].numpy()\n",
    "    actuals = actuals[mask].numpy()\n",
    "\n",
    "    preds[:, :4] = scaler.inverse_transform(preds[:, :4])\n",
    "    actuals[:, :4] = scaler.inverse_transform(actuals[:, :4])\n",
    "    preds[:, 4] = scaler1.inverse_transform(preds[:, 4].reshape(-1, 1)).reshape(-1)\n",
    "    actuals[:, 4] = scaler1.inverse_transform(actuals[:, 4].reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "    mae = mean_absolute_error(actuals, preds)\n",
    "    rmse = np.sqrt(mean_squared_error(actuals, preds))\n",
    "    mape = np.mean(np.abs((actuals - preds) / actuals)) * 100\n",
    "\n",
    "    print(f\"\\nMAE: {mae:.4f}, RMSE: {rmse:.4f}, MAPE: {mape:.2f}%\")\n",
    "    return mae, rmse, mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "evaluate() takes 5 positional arguments but 6 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index_full\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_tickers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: evaluate() takes 5 positional arguments but 6 were given"
     ]
    }
   ],
   "source": [
    "evaluate(model, val_dates, df1, edge_index_full, all_tickers, \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
